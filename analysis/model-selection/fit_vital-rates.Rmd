---
title: "Mixed Model Selection"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
---

# Initialization

```{r}
library(milkweed)
library(dplyr)
library(tidyr)
library(ggplot2)
library(AICcmodavg)
library(vegan)
```

# Exploratory Model Selection

We want to know if including any of these traits (and/or herbivory) ever predicts the vital rates better than just height alone. ... first, N LMA ADL and cardenolides

## Flowering
```{r}
# full.data <- read.csv("../../inst/extdata/fulldata.csv")
flower.data = full.data %>% filter(!is.na(h_apical), !is.na(fec.flower))
```

Let's make some plots to see what the data looks like.
```{r}
f1 = ggplot(flower.data, aes(x = h_apical, y = fec.flower)) + 
            geom_jitter(height = 0.1, alpha = 0.3, color = 'blue')

f2 = ggplot(flower.data, aes(x = herb_avg, y = fec.flower)) + 
            geom_jitter(height = 0.1, alpha = 0.3, color = 'blue')

f3 = ggplot(flower.data, aes(x = N, y = fec.flower)) + 
            geom_jitter(height = 0.1, alpha = 0.3, color = 'blue')

f4 = ggplot(flower.data, aes(x = LMA, y = fec.flower)) + 
            geom_jitter(height = 0.1, alpha = 0.3, color = 'blue')

f5 = ggplot(flower.data, aes(x = ADL, y = fec.flower)) + 
            geom_jitter(height = 0.1, alpha = 0.3, color = 'blue')

f6 = ggplot(flower.data, aes(x = Cardenolides, y = fec.flower)) + 
            geom_jitter(height = 0.1, alpha = 0.3, color = 'blue')

flower.plots = gridExtra::grid.arrange(f1, f2, f3, f4, f5, f6)
```

Here's the model with height as the only predictor to compare against.
```{r}
flower.null = glm(fec.flower ~ h_apical, data = flower.data, family = binomial())
```

Here are the models that contain height and 1 other additional predictor. We want to see if any of these are a better fit than height alone.
```{r}
flower.herb = glm(fec.flower ~ h_apical + herb_avg, data = flower.data, family = binomial())
flower.N = glm(fec.flower ~ h_apical + N, data = flower.data, family = binomial())
flower.LMA = glm(fec.flower ~ h_apical + LMA, data = flower.data, family = binomial())
flower.ADL = glm(fec.flower ~ h_apical + ADL, data = flower.data, family = binomial())
flower.Card = glm(fec.flower ~ h_apical + Cardenolides, data = flower.data, family = binomial())

aictab(list(flower.null, flower.herb, flower.N, flower.LMA, flower.ADL, flower.Card), modnames = c('null', 'herb', 'N', 'LMA', 'ADL', 'Cardenolides'))
```

It looks like all of them are a better fit than height alone (except N)! And Cardenolides is even better than herb_avg as an additional predictor. Let's try combining with herb_avg to see if that gives us a better combination, since we know herb is also a good predictor. 
```{r}
flower.hN = glm(fec.flower ~ h_apical + herb_avg + N, data = flower.data, family = binomial())
flower.hLMA = glm(fec.flower ~ h_apical + herb_avg + LMA, data = flower.data, family = binomial())
flower.hADL = glm(fec.flower ~ h_apical + herb_avg + ADL, data = flower.data, family = binomial())
flower.hCard = glm(fec.flower ~ h_apical + herb_avg + Cardenolides, data = flower.data, family = binomial())

# We can also try this, since these we think these are the two most promising.
flower.CardLMA = glm(fec.flower ~ h_apical + Cardenolides + LMA, data = flower.data, family = binomial())

aictab(list(flower.herb, flower.hN, flower.hLMA, flower.hADL, flower.hCard, flower.Card, flower.LMA, flower.CardLMA), modnames = c('herb', 'N + herb', 'LMA + herb', 'ADL + herb', 'Cardenolides + herb', 'Cardenolides alone', 'LMA alone',  'Card + LMA'))
```

Let's try adding in some interactions.
```{r}
flower.Card.full = glm(fec.flower ~ h_apical*Cardenolides, data = flower.data, family = binomial())
flower.CardLMA.full = glm(fec.flower ~ h_apical*Cardenolides*LMA, data = flower.data, family = binomial())
flower.hCard.full = glm(fec.flower ~ h_apical*Cardenolides*herb_avg, data = flower.data, family = binomial())

aictab(list(flower.herb, flower.hCard, flower.Card, flower.CardLMA, flower.Card.full, flower.CardLMA.full, flower.hCard.full), modnames = c('herb', 'Cardenolides + herb', 'Cardenolides alone', 'Card + LMA', 'Cardenolides full', 'Card + LMA full', "Card + herb full"))
```

Let's see if some of these are correlated.
```{r}
plot(Cardenolides ~ h_apical, data = flower.data)
plot(LMA ~ h_apical, data = flower.data)
plot(herb_avg ~ Cardenolides, data = flower.data)

cor.test(flower.data$Cardenolides, flower.data$h_apical)
cor.test(flower.data$LMA, flower.data$h_apical)
cor.test(flower.data$Cardenolides, flower.data$herb_avg)
```

## Survival
```{r}
surv.data = full.data %>% filter(!is.na(h_apical), !is.na(surv))
```

Let's make some plots to see what the data looks like.
```{r}
s1 = ggplot(surv.data, aes(x = h_apical, y = surv)) + 
            geom_jitter(height = 0.1, alpha = 0.3, color = 'blue')      # Interesting that this plot doesn't suggest much of a correlation

s2 = ggplot(surv.data, aes(x = herb_avg, y = surv)) + 
            geom_jitter(height = 0.1, alpha = 0.3, color = 'blue')

s3 = ggplot(surv.data, aes(x = N, y = surv)) + 
            geom_jitter(height = 0.1, alpha = 0.3, color = 'blue')

s4 = ggplot(surv.data, aes(x = LMA, y = surv)) + 
            geom_jitter(height = 0.1, alpha = 0.3, color = 'blue')

s5 = ggplot(surv.data, aes(x = ADL, y = surv)) + 
            geom_jitter(height = 0.1, alpha = 0.3, color = 'blue')

s6 = ggplot(surv.data, aes(x = Cardenolides, y = surv)) + 
            geom_jitter(height = 0.1, alpha = 0.3, color = 'blue')

surv.plots = gridExtra::grid.arrange(s1, s2, s3, s4, s5, s6)
```

Height doesn't look like it's doing so hot here. Maybe we should test it against other stuff before just including it by default.
```{r}
surv.null = glm(surv ~ h_apical, data = surv.data, family = binomial())
surv.null.herb = glm(surv ~ herb_avg, data = surv.data, family = binomial())
surv.null.N = glm(surv ~ N, data = surv.data, family = binomial())
surv.null.LMA = glm(surv ~ LMA, data = surv.data, family = binomial())
surv.null.ADL = glm(surv ~ ADL, data = surv.data, family = binomial())
surv.null.Card = glm(surv ~ Cardenolides, data = surv.data, family = binomial())

aictab(list(surv.null, surv.null.herb, surv.null.N, surv.null.LMA, surv.null.ADL, surv.null.Card), modnames = c('height', 'herb (no height)', 'N (no height)', 'LMA (no height)', 'ADL (no height)', 'Card (no height)'))
```

Wow, height seems to be among the worst! No surprise that Cardenolides, herb, and LMA are the top 3. Let's use Cardenolides (no height) as the null this time.
```{r}
surv.null = surv.null.Card

surv.herb = glm(surv ~ Cardenolides + herb_avg, data = surv.data, family = binomial())
surv.N = glm(surv ~ Cardenolides + N, data = surv.data, family = binomial())
surv.LMA = glm(surv ~ Cardenolides + LMA, data = surv.data, family = binomial())
surv.ADL = glm(surv ~ Cardenolides + ADL, data = surv.data, family = binomial())
surv.height = glm(surv ~ Cardenolides + h_apical, data = surv.data, family = binomial())

aictab(list(surv.null, surv.herb, surv.N, surv.LMA, surv.ADL, surv.height), modnames = c('null', 'herb', 'N', 'LMA', 'ADL', 'height'))
```

Cardenolides and herb is once again the best combo, followed by Card and LMA.
```{r}
surv.hLMA = glm(surv ~ Cardenolides + herb_avg + LMA, data = surv.data, family = binomial())
surv.herb.full = glm(surv ~ Cardenolides*herb_avg, data = surv.data, family = binomial())
surv.LMA.full = glm(surv ~ Cardenolides*LMA, data = surv.data, family = binomial())
surv.hLMA.full = glm(surv ~ Cardenolides*herb_avg*LMA, data = surv.data, family = binomial())

aictab(list(surv.herb, surv.LMA, surv.hLMA, surv.herb.full, surv.LMA.full, surv.hLMA.full), modnames = c('herb + Card', 'LMA + Card', 'herb + Card + LMA', 'herb + Card full', 'LMA + Card full', 'herb + Card + LMA full'))
```

Not including the interactions seems to be the best here. Let's see if Card and LMA are correlated.
```{r}
plot(LMA ~ Cardenolides, data = surv.data)

cor.test(surv.data$LMA, surv.data$Cardenolides)
```
So LMA and Cardenolides are somewhat negatively correlated. The Cor is starting to approach +-0.5 where it starts getting questionable, bu thas not gotten there yet.

## Growth
```{r}
growth.data = full.data %>% filter(!is.na(h_apical), !is.na(h_apical.next))
```

```{r}
g1 = ggplot(growth.data, aes(x = h_apical, y = h_apical.next)) + 
            geom_point(alpha = 0.3, color = 'blue')      

g2 = ggplot(growth.data, aes(x = herb_avg, y = h_apical.next)) + 
            geom_point(alpha = 0.3, color = 'blue')

g3 = ggplot(growth.data, aes(x = N, y = h_apical.next)) + 
            geom_point(alpha = 0.3, color = 'blue')

g4 = ggplot(growth.data, aes(x = LMA, y = h_apical.next)) + 
            geom_point(alpha = 0.3, color = 'blue')

g5 = ggplot(growth.data, aes(x = ADL, y = h_apical.next)) + 
            geom_point(alpha = 0.3, color = 'blue')

g6 = ggplot(growth.data, aes(x = Cardenolides, y = h_apical.next)) + 
            geom_point(alpha = 0.3, color = 'blue')

growth.plots = gridExtra::grid.arrange(g1, g2, g3, g4, g5, g6)
```

```{r}
growth.null = lm(h_apical.next ~ h_apical, data = growth.data) #obvious choice for the null here
```

```{r}
growth.herb = lm(h_apical.next ~ h_apical + herb_avg, data = growth.data)
growth.N = lm(h_apical.next ~ h_apical + N, data = growth.data)
growth.LMA = lm(h_apical.next ~ h_apical + LMA, data = growth.data)
growth.ADL = lm(h_apical.next ~ h_apical + ADL, data = growth.data)
growth.Card = lm(h_apical.next ~ h_apical + Cardenolides, data = growth.data)

aictab(list(growth.null, growth.herb, growth.N, growth.LMA, growth.ADL, growth.Card), modnames = c('null', 'herb', 'N', 'LMA', 'ADL', 'Cardenolides'))
```

Wow, Cardenolides is standing out a lot. Let's throw in some interactions.
```{r}
growth.Card.full = lm(h_apical.next ~ h_apical*Cardenolides, data = growth.data)

aictab(list(growth.null, growth.Card, growth.Card.full), modnames = c('null', 'Cardenolides + height', 'Card + height full'))
```
Adding in the interaction term doesn't improve it, and we've already done the cor test and seen no significant correlation.

## Pods
```{r}
pods.data = full.data %>% filter(!is.na(h_apical), !is.na(N_pods))
```

```{r}
p1 = ggplot(pods.data, aes(x = h_apical, y = N_pods)) + 
            geom_point(alpha = 0.3, color = 'blue')      

p2 = ggplot(pods.data, aes(x = herb_avg, y = N_pods)) + 
            geom_point(alpha = 0.3, color = 'blue')

p3 = ggplot(pods.data, aes(x = N, y = N_pods)) + 
            geom_point(alpha = 0.3, color = 'blue')

p4 = ggplot(pods.data, aes(x = LMA, y = N_pods)) + 
            geom_point(alpha = 0.3, color = 'blue')

p5 = ggplot(pods.data, aes(x = ADL, y = N_pods)) + 
            geom_point(alpha = 0.3, color = 'blue')

p6 = ggplot(pods.data, aes(x = Cardenolides, y = N_pods)) + 
            geom_point(alpha = 0.3, color = 'blue')

pods.plots = gridExtra::grid.arrange(p1, p2, p3, p4, p5, p6)
```

Height and N, look good for a Poisson. Herb could arguably be exponential...

```{r}
pods.null = glm(N_pods ~ h_apical, data = pods.data, family = poisson())
```

```{r}
pods.herb = glm(N_pods ~ h_apical + herb_avg, data = pods.data, family = poisson())
pods.N = glm(N_pods ~ h_apical + N, data = pods.data, family = poisson())
pods.LMA = glm(N_pods ~ h_apical + LMA, data = pods.data, family = poisson())
pods.ADL = glm(N_pods ~ h_apical + ADL, data = pods.data, family = poisson())
pods.Card = glm(N_pods ~ h_apical + Cardenolides, data = pods.data, family = poisson())

aictab(list(pods.null, pods.herb, pods.N, pods.LMA, pods.ADL, pods.Card), modnames = c('null', 'herb', 'N', 'LMA', 'ADL', 'Cardenolides'))
```

Wait, what... why is Cardenolides so high? That doesn't make sense looking at the data. We are dubious... but it has no weight anyway, so it really isn't very close.

# Principal Component Analysis

So before we go any further with this, we should get a real look at the correlations and variance in the data to get a feel for which variables in our set are quantitatively the most important. We have judged which we think will be intuitively the most important, so now let's see if this matches up. First thing to do is some ordination using PCA to determine which axes through our data space are most important. Following from Numerical Ecology ed.2 by Borcard et al.

```{r}
#Not including herbivory or height in this as we are just concerned with how the traits are related to each other.
pca.data = full.data %>% filter(!is.na(LMA), !is.na(Cardenolides)) %>% select(N, C, LMA, ADF, ADL, Cellulose, NDWI, Chl_g_m2, PRI, Cardenolides) #removing NAs that interfere with the pca and selecting only relevant data columns

#scaling all the columns of the data, so that we can work with a correlation matrix rather than a covariance matrix and don't have to worry about the scale of the variables influencing their weights in the ordination
pca.data = scale(pca.data)
```

Let's run the pca and look at some summary information.
```{r}
pca.traits = rda(pca.data)
summary(pca.traits, scaling = 2)
```

Now let's apply and visualize some criteria for selecting 'important' axes.
```{r}
ev = pca.traits$CA$eig

#apply Kaiser-Guttman criterion (only those axes whose eigenvalues are above the mean eigenvalue are considered important)
ev[ev > mean(ev)]

#apply Broken stick model (only those axes whose eigenvalue is larger than the corresponding random length of stick, p.122 Borcard et al 2011)
n = length(ev)
bsm = data.frame(j=seq(1:n), p=0)
bsm$p[1] <- 1/n
for (i in 2:n) {
	bsm$p[i] = bsm$p[i-1] + (1/(n + 1 - i))
}
bsm$p = 100*bsm$p/n
bsm

# Plot eigenvalues and % of variance for each axis
dev.new(title="PCA eigenvalues")
par(mfrow=c(2,1))
barplot(ev, main="Eigenvalues", col="bisque", las=2)
abline(h=mean(ev), col="red")		# average eigenvalue
legend("topright", "Average eigenvalue", lwd=1, col=2, bty="n")
barplot(t(cbind(100*ev/sum(ev),bsm$p[n:1])), beside=TRUE, main="% variance", col=c("bisque",2), las=2)
legend("topright", c("% eigenvalue", "Broken stick model"), pch=15, col=c("bisque",2), bty="n")
```

So it appears that our first 2 axes are most important by the Kaiser-Guttman Criterion, and the first 2 are important by the BSM. I believe the BSM to be more conservative, so we will keep the first 2 in mind. Let's look at a plot of the ordination.
```{r}
#biplot.rda(pca.traits, main='PCA - Scaling 2') #using vegan's biplot.rda() function, which is missing for some reason, scaling 2 is optimal display for variables, the angles between descriptors in the plot reflect their correlations

#using instead a function written by Borcard et al 2011 for doing just this task.
source('cleanplot.pca.R') #cleanplot file included in source directory
cleanplot.pca(pca.traits, scaling = 1, plot.sites = T, plot.spe = T, label.sites = F, label.spe = T)
cleanplot.pca(pca.traits, plot.sites = T, plot.spe = T, label.sites = F, label.spe = T) #including points for sites (plants), and arrowheads and labels on vectors, scaling 2 is default. Distances are meaningless.
```

So looks like PCA1 captures the tradeoff between defense (cardenolides) and growth/resproduciton (C, cellulose, fiber, lignin) and PCA2 captures photsynthetic information (N, PRI, Chl_g_m2). This is awesome!!

## Tradeoffs
Let's look further at this tradeoff between growth and defense.
```{r}
trade.off.data = mutate(full.data, h_diff = h_apical.next - h_apical)
trade.off.1 = lm(Cellulose ~ Cardenolides, data=trade.off.data)
summary(trade.off.1)
trade.off.2 = lm(ADL ~ Cardenolides, data=trade.off.data)
summary(trade.off.2)
trade.off.3 = lm(ADF ~ Cardenolides, data=trade.off.data)
summary(trade.off.3)
trade.off.4 = lm(LMA ~ Cardenolides, data=trade.off.data)
summary(trade.off.4)
```

So Cardenolides have a highly significant negative relationship with all the structural traits. Let's see if this holds up in differences in height.
```{r}
trade.diff = lm(h_diff ~ Cardenolides, data=trade.off.data)
summary(trade.diff)
```


# Modelling Vital Rates with Traits
So now that we have these PC axes, we would like to see if they can be used as predictors variables in the model, and compare them to using LMA, N and Cardenolides since the PCA indicated that these were the most orthogonal and the exploratory model seletion showed the most promise in these variables. We are doing this by finding the best model from two sets of predictors: raw trait variables (N, LMA, Cardenolides + h_apical and herb_avg), and pc variables (PC1, PC2 + h_apical and herb_avg), then comparing these two best models with AIC to determine which is better.

## Data
Move some data around so we can use the PC axes as variables.
```{r}
#switching to prcomp for ease of retrieving info, pca is the same
prcomp.traits = prcomp(pca.data) #data have already been scaled

PC1 = prcomp.traits$x[,1]
PC2 = prcomp.traits$x[,2]

model.data = full.data %>% filter(!is.na(LMA), !is.na(Cardenolides)) %>%
                           select(-c(C, ADF, ADL, Cellulose, NDWI, PRI, Chl_g_m2))
model.data = bind_cols(model.data, data.frame(PC1, PC2))
```

## Investigating Potential Interactions
Next, we want to look at whether or not there might be relevant interactions between some of these variables that would make sense to include in the models of the demographic rates. To do this, we will model herb as functions of N and Cardenolides and compare the combinations. If the N*Cardenolides model has a significant relationship with herb_avg then this would indicate that there is a relationship between N and Card in determining the herbivory score (ie. high N, low Card might be the highest herb; or low N, high Card the lowest)...  but our herbivory score might not be on a fine enough scale to capture these relationships, so it might not appear in our data.

First we would like to know whether or not herbivory should be included in the models at all. To decide this, we will look at the correlations between the vatriables and herbivory when we exclude the 0s for herb_avg. The thought behind this is thus: if there is a correlation between the traits and herbivory score (given that they have been eaten), we can exclude it from the model for those plants since this information will be present in the trait data. For the plants with 0 herbivory, herb_avg is a dummy variable that tells us nothing, since they are all the same, therefore it can be excluded for those plants as well... If there is no correlation even after filtering the 0s, then we will keep herb_avg as a candidate predictor in the models because it might provid more information.
```{r}
model.data.munch = model.data %>% filter(herb_avg > 0)
```

Now we should check to see some correlations.
```{r}
model.cors.munch = cor(select(model.data.munch, h_apical, herb_avg, N:PC2), use = 'complete.obs')
model.cors.munch #ignore PCs except wrt height and herb
```
Nothing is very highly correlated when munch filter is applied, except (maybe?) LMA and CArdenolides, which never appear in the same model anyway. There is not much correlation between N or Cardenloides and herb_avg.

Okay, now onto the potential interactions...First let's plot some data to see what they look like.
```{r}
plot(herb_avg ~ N, data=model.data)
plot(herb_avg ~ Cardenolides, data=model.data)
```

```{r}
herb.N = lm(herb_avg ~ N, data=model.data)
herb.Card = lm(herb_avg ~ Cardenolides, data=model.data)
herb.Both = lm(herb_avg ~ N + Cardenolides, data=model.data)
herb.Int = lm(herb_avg ~ N*Cardenolides, data=model.data)

aictab(list(herb.N, herb.Card, herb.Both, herb.Int), modnames = c('N', 'Cardenolides', 'Both', 'Interactions'))
```
Including interactions does not make the model any better.

Do the same process for PC1 and PC2 as proxies for Card and N, respectively.
```{r}
herb.1 = lm(herb_avg ~ PC1, data=model.data)
herb.2 = lm(herb_avg ~ PC2, data=model.data)
herb.Both2 = lm(herb_avg ~ PC1 + PC2, data=model.data)
herb.Int2 = lm(herb_avg ~ PC1*PC2, data=model.data)

aictab(list(herb.1, herb.2, herb.Both2, herb.Int2), modnames = c('PC1', 'PC2', 'Both', 'Interactions'))
```
Including interactions actually makes it worse.

Now height.
```{r}
height.N = lm(h_apical ~ N, data=model.data)
height.Card = lm(h_apical ~ Cardenolides, data=model.data)
height.Both = lm(h_apical ~ N + Cardenolides, data=model.data)
height.Int = lm(h_apical ~ N*Cardenolides, data=model.data)

aictab(list(height.N, height.Card, height.Both, height.Int), modnames = c('N', 'Cardenolides', 'Both', 'Interactions'))
```
Including interactions does not make the model any better.

Do the same process for PC1 and PC2 as proxies for Card and N, respectively.
```{r}
height.1 = lm(h_apical ~ PC1, data=model.data)
height.2 = lm(h_apical ~ PC2, data=model.data)
height.Both2 = lm(h_apical ~ PC1 + PC2, data=model.data)
height.Int2 = lm(h_apical ~ PC1*PC2, data=model.data)

aictab(list(height.1, height.2, height.Both2, height.Int2), modnames = c('PC1', 'PC2', 'Both', 'Interactions'))
```
Hmm... using PC axes, we get an interaction, but not when using raw traits... not including interactions for now.

## Vital Rate Model Selection
### Flowering
```{r}
flower.m.data = model.data %>% filter(!is.na(fec.flower))
```

Backward model selection method for raw traits predictor set.
```{r}
flower.raw.full = glm(fec.flower ~ h_apical + herb_avg + N + LMA + Cardenolides, data = flower.m.data, family = binomial())
drop1(flower.raw.full)
flower.raw.1 = glm(fec.flower ~ h_apical + herb_avg + LMA + Cardenolides, data = flower.m.data, family = binomial())
drop1(flower.raw.1)
flower.raw.2 = glm(fec.flower ~ h_apical + LMA + Cardenolides, data = flower.m.data, family = binomial())
drop1(flower.raw.2)
flower.raw.2.1 = glm(fec.flower ~ h_apical + herb_avg + Cardenolides, data = flower.m.data, family = binomial()) #since droptable is weird, this would be the next best to drop
drop1(flower.raw.2.1)

flower.raw.3 = glm(fec.flower ~ h_apical + Cardenolides, data = flower.m.data, family = binomial()) #dropping herb_avg doesn't actually lower AIC
drop1(flower.raw.3)

aictab(list(flower.raw.full, flower.raw.1, flower.raw.2, flower.raw.2.1, flower.raw.3), modnames = c('full', '1', '2', '2.1', '3'))

## drop table not matching up... so 2 and 3 are actually worse than 2.1 when you do the AIC by hand (AIC()), so sticking with 2.1
flower.raw.final = flower.raw.2.1
summary(flower.raw.final)                   
```

Backward model selection for PCA predictor set.
```{r}
flower.pc.full = glm(fec.flower ~ h_apical + herb_avg + PC1 + PC2, data = flower.m.data, family = binomial())
drop1(flower.pc.full)
flower.pc.1 = glm(fec.flower ~ h_apical + PC1 + PC2, data = flower.m.data, family = binomial())
drop1(flower.pc.1)
flower.pc.1.1 = glm(fec.flower ~ h_apical + herb_avg + PC1, data = flower.m.data, family = binomial())
drop1(flower.pc.1.1)

## 2 is worse than 1, so keeping herb_avg
flower.pc.final = flower.pc.1
summary(flower.pc.final)
```

Now we will compare the best models from the two predictor sets with AIC.
```{r}
aictab(list(flower.raw.final, flower.pc.final), modnames= c('Raw Plant Traits', 'PC Axes'))
```

So from this we can see that the raw plant traits produced a best model with lower AIC.

### Survival
```{r}
surv.m.data = model.data %>% filter(!is.na(surv))
```

Backward model selection method for raw traits predictor set.
```{r}
surv.raw.full = glm(surv ~ h_apical + herb_avg + N + LMA + Cardenolides, data = surv.m.data, family = binomial())
drop1(surv.raw.full)
surv.raw.1 = glm(surv ~ h_apical + herb_avg + LMA + Cardenolides, data = surv.m.data, family = binomial())
drop1(surv.raw.1)

surv.raw.final = surv.raw.1
summary(surv.raw.final)
```

Backward model selection for PCA predictor set.
```{r}
surv.pc.full = glm(surv ~ h_apical + herb_avg + PC1 + PC2, data = surv.m.data, family = binomial())
drop1(surv.pc.full)
surv.pc.1 = glm(surv ~ h_apical + herb_avg + PC1, data = surv.m.data, family = binomial()) #dropping PC2 to compare a simpler model
drop1(surv.pc.1)

## full better than 1, so keeping PC2
surv.pc.final = surv.pc.full
summary(surv.pc.final)
```

Now we will compare the best models from the two predictor sets with AIC.
```{r}
aictab(list(surv.raw.final, surv.pc.final), modnames= c('Raw Plant Traits', 'PC Axes'))
```

So from this we can see that the PC traits produced a best model with lower AIC... although indistinguishable.

### Growth
```{r}
growth.m.data = model.data %>% filter(!is.na(h_apical), !is.na(h_apical.next))
```

Backward model selection method for raw traits predictor set.
```{r}
growth.raw.full = lm(h_apical.next ~ h_apical + herb_avg + N + LMA + Cardenolides, data = growth.m.data)
drop1(growth.raw.full)
growth.raw.1 = lm(h_apical.next ~ h_apical + herb_avg + N + Cardenolides, data = growth.m.data) #indistinguishable, so comparing simpler model
drop1(growth.raw.1)

#nothing better emerged, so sticking with full
growth.raw.final = growth.raw.full
summary(growth.raw.final)
```

Backward model selection method for PCA predictor set.
```{r}
growth.pc.full = lm(h_apical.next ~ h_apical + herb_avg + PC1 + PC2, data = growth.m.data)
drop1(growth.pc.full)

growth.pc.final = growth.pc.full
summary(growth.pc.final)
```

Now we will compare the best models from the two predictor sets with AIC.
```{r}
aictab(list(growth.raw.final, growth.pc.final), modnames= c('Raw Plant Traits', 'PC Axes'))
```

So from this we can see that the PC traits produced a best model with distinguishably lower AIC...

### Pods
```{r}
pods.m.data = model.data %>% filter(!is.na(N_pods))
```

Comparison of June vs September Height.
```{r}
pods.raw.full.j = glm(N_pods ~ h_apical + herb_avg + N + LMA + Cardenolides, data = pods.m.data, family = poisson())
pods.raw.full.s = glm(N_pods ~ h_apical.next + herb_avg + N + LMA + Cardenolides, data = pods.m.data, family = poisson())
aictab(list(pods.raw.full.j, pods.raw.full.s), modnames = c('June', 'September'))
```

Hmmm... so now June height is a comparable predictor of pod production to September height... this means we can make a simpler IPM and a change in the life cycle diagram (removing growth and tau, which should make the IPM simpler).

Backward model selection method for raw traits predictor set.
```{r}
pods.raw.full = glm(N_pods ~ h_apical + herb_avg + N + LMA + Cardenolides, data = pods.m.data, family = poisson())
drop1(pods.raw.full)
pods.raw.1 = glm(N_pods ~ h_apical + herb_avg + LMA + Cardenolides, data = pods.m.data, family = poisson())
drop1(pods.raw.1)

pods.raw.final = pods.raw.1
summary(pods.raw.final)
```

Backward model selection method for PCA predictor set.
```{r}
pods.pc.full = glm(N_pods ~ h_apical + herb_avg + PC1 + PC2, data = pods.m.data, family = poisson())
drop1(pods.pc.full)
pods.pc.1 = glm(N_pods ~ h_apical + herb_avg + PC1, data = pods.m.data, family = poisson())
drop1(pods.pc.1)

pods.pc.final = pods.pc.1
summary(pods.pc.final)
```

Now we will compare the best models from the two predictor sets with AICc.
```{r}
aictab(list(pods.raw.final, pods.pc.final), modnames= c('Raw Plant Traits', 'PC Axes'))
```

So from this we can see that the raw plant traits produced a best model with lower AICc.
